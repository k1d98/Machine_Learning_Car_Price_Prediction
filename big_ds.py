# -*- coding: utf-8 -*-
"""big_ds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KlbQPjdktdyJKxCeD7REe623HKbKUz4

Dataset link:
https://www.kaggle.com/austinreese/craigslist-carstrucks-data

#Imports
"""

import pandas as pd
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
import seaborn as sns 
from scipy.stats import binom_test
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.dummy import DummyRegressor
from 	sklearn.metrics import max_error
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import explained_variance_score
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
from patsy import dmatrices
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.dummy import DummyRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
from sklearn.decomposition import PCA
from sklearn.svm import SVR
from sklearn.svm import LinearSVR
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from google.colab import drive
drive.mount('/content/drive')

"""#Data"""

df = pd.read_csv('/content/drive/My Drive/ML/Project/vehicles.csv', encoding='ISO-8859-1')

# Commented out IPython magic to ensure Python compatibility.
num_attributes = ["price", "year", "odometer"]
# %matplotlib inline
pd.plotting.scatter_matrix(df[num_attributes], figsize = (12,8), alpha = 0.1)

for col in df.columns: 
    print(col) 
df.describe()

df.drive.value_counts()

rule = df[(df['price'] > 100000) | (df['price'] < 750)].index
df.drop(rule, inplace=True)

rule2 = df[(df['odometer'] < 10) | (df['odometer']> 350000)].index
df.drop(rule2, inplace=True)

rule4 = df[(df['year'] < 1985) ].index
df.drop(rule4, inplace=True)


rule3 = df[(df['title_status'] != 'clean')].index
df.drop(rule3, inplace=True)
#drop title status because it's same everywhere
df =df.drop(['title_status'],1)

rule5 = df[(df['condition'] == 'new') | (df['condition'] == 'salvage')].index
df.drop(rule5, inplace=True)
df =df.drop(['condition'],1)

# Commented out IPython magic to ensure Python compatibility.
num_attributes = ["price", "year", "odometer"]
# %matplotlib inline
pd.plotting.scatter_matrix(df[num_attributes], figsize = (12,8), alpha = 0.1)

df = df.drop(['size','transmission','model','id','url','region','region_url','vin','image_url','paint_color','description','county','state','lat','long'],1)

print(df.isnull().sum())
df = df.replace(0, np.nan)

df = df.replace('NaN', np.nan)
df = df.replace('nan', np.nan)
df['cylinders'].fillna(value='not-declared', inplace=True)
df['fuel'].fillna(value='gas', inplace=True)
df['type'].fillna(value='not-declared', inplace=True)
df['drive'].fillna(value='fwd', inplace=True)
#df['odometer'].fillna(value=df['odometer'].mean(), inplace=True)
print(df.isnull().sum())
#df['condition'] =df['condition'].dropna()
#df = df.ffill(axis = 0)
df=df.dropna();
print(df.isnull().sum())

print(df.shape)

y=df["price"]
x=df.drop(["price"],1)
x.shape

categorical_f = ['manufacturer','fuel','type','drive','cylinders']

numerical_f= list(x.drop(categorical_f, axis=1))

categorical_pipe = Pipeline([
    ('onehot', OneHotEncoder())
])
numerical_pipe = Pipeline([
    ("polynomial_features",PolynomialFeatures(2)),
    ("std_scaler", StandardScaler())
])

preprocessing = ColumnTransformer(
    [('cat', categorical_pipe, categorical_f),
     ('num', numerical_pipe, numerical_f)])

x = preprocessing.fit_transform(x)

#x = SelectKBest(f_regression, k=70).fit_transform(x, y)

resultsarray=[]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""# Metrics"""

def metrics(predictions, y_t,name):
  errors = abs(predictions - y_t)
  mae=round(mean_absolute_error(y_t,predictions),2)
  print('Mean abs error: ', mae)
  maxe=max_error(predictions,y_t)
  print('Max eror: ',maxe)
  mape = 100 * (errors / y_t)
  # Calculate and display accuracy
  accuracy = 100 - np.mean(mape)
  acc=round(accuracy, 2)
  print('Accuracy:',acc , '%.')
  r2=r2_score(y_t, predictions)
  print('R2 ', r2)
  evs=explained_variance_score(predictions,y_t, multioutput='uniform_average')
  print('Explained variance score: ',evs)
  resultsarray.append([name,mae,maxe,acc,r2,evs])

"""# Random Forest Regressor"""

model = RandomForestRegressor(n_jobs=-1,n_estimators=98,random_state=0,verbose=True,)
model.fit(X_train, y_train)

print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Random fores regressor train')
print("TEST: \n")
metrics( model.predict(X_test), y_test, 'Random forest regressor test')

"""# Decision Tree Regressor"""

model = DecisionTreeRegressor(max_depth=60,random_state=0,splitter='random')
model.fit(X_train,y_train)

print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Decision tree regressor train')
print("TEST: \n")
metrics( model.predict(X_test), y_test,'Decision tree regressor test tuning')

"""# SGDRegressor instead of SVR

SVMs are known to scale badly with the number of samples! The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. 

Instead of SVR with a linear-kernel, use LinearSVR or for huge data: SGDClassifier
"""

model = LinearSVR(random_state=0, tol=1e-5,epsilon=0.8, C=2000)
model.fit(X_train, y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Linear SVR train')
print("TEST: \n")
metrics( model.predict(X_test), y_test,'Linear SVR test')

'''
Probably because of the size of this dataset, this model didn't give results in adequate time
model = SVR(kernel='poly', epsilon=0.8, C=2000)
model.fit(X_train, y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train)
print("TEST: \n")
metrics( model.predict(X_test), y_test)
'''

"""# Linear Regression"""

model=LinearRegression()
model.fit(X_train,y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Linear regression train')
print("TEST: \n")
metrics( model.predict(X_test), y_test,'Linear regression test')

"""# Neural Network"""

from sklearn.decomposition import PCA
pca = PCA(60)
X_train=X_train.toarray()
pca.fit(X_train)

X_train = pca.transform(X_train)
X_test=X_test.toarray()
X_test = pca.transform(X_test)
print(X_train.shape)

NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))


# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

NN_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)

# Load wights file of the best model :
wights_file = 'Weights-099--2402.94804.hdf5' # choose the best checkpoint 
NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

predictions = NN_model.predict(X_train)
print("TRAIN: \n")
metrics( predictions[:,0], y_train,'Neural net train')
print("TEST: \n")
predictions = NN_model.predict(X_test)
metrics( predictions[:,0], y_test,'Neural net test')

"""#DUMMY"""

model = DummyRegressor(strategy="median")
model.fit(X_train, y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Dummy train')
print("TEST: \n")
metrics( model.predict(X_test), y_test,'Dummy test')

"""# All results"""

results_df= pd.DataFrame(np.array(resultsarray),columns=['Model name','Mean abs error','Max error','Accuracy','R2','Explained variance score'])

results_df

'''
pipe = Pipeline([("skb",SelectKBest()),("model",DecisionTreeRegressor())])
grid = GridSearchCV(pipe, {"skb__k": [8,10,15,20,40,60]})
grid.fit(X_train,y_train)
print('Best params on:\n')
print(grid.best_params_)'''