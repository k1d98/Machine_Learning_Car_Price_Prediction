# -*- coding: utf-8 -*-
"""small_ds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVOykyKdkZQ6PZsWhsOjT4ZmB9m7S58z

# Dataset

https://www.kaggle.com/antfarol/car-sale-advertisements

# Imports
"""

import pandas as pd
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
import seaborn as sns 
from scipy.stats import binom_test
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.dummy import DummyRegressor
from 	sklearn.metrics import max_error
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import explained_variance_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
from patsy import dmatrices
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.dummy import DummyRegressor

from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from sklearn.metrics import mean_absolute_error
from matplotlib import pyplot as plt
import seaborn as sb
from sklearn.decomposition import PCA
from sklearn.svm import SVR
from sklearn.svm import LinearSVR

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

from sklearn.preprocessing import OrdinalEncoder

from google.colab import drive
drive.mount('/content/drive')

"""# Data preprocessing"""

df = pd.read_csv('/content/drive/My Drive/ML/Project/car_ad.csv', encoding='ISO-8859-1')

for col in df.columns: 
    print(col) 
df.describe()

print(df)

# Commented out IPython magic to ensure Python compatibility.
num_attributes = ["price", "year", "engV", "mileage"]
# %matplotlib inline
pd.plotting.scatter_matrix(df[num_attributes], figsize = (12,8), alpha = 0.1)

rule = df[(df['price'] > 120000) | (df['price'] < 750)].index
df.drop(rule, inplace=True)

rule2 = df[(df['mileage'] < 10) | (df['mileage']> 400)].index
df.drop(rule2, inplace=True)

rule3 = df[(df['engV'] > 6.5) ].index
df.drop(rule3, inplace=True)

rule3 = df[(df['year'] < 1975) ].index
df.drop(rule3, inplace=True)

df.describe()

# Commented out IPython magic to ensure Python compatibility.
num_attributes = ["price", "year", "engV", "mileage"]
# %matplotlib inline
pd.plotting.scatter_matrix(df[num_attributes], figsize = (12,8), alpha = 0.1)

# experiments to combine features gave worse results:
#x['engV'] = x['mileage']/x['engV']
df =df.drop(['model','mileage','engType'],1)
print(df.shape)

print('before deleting nulls')
print(df.isnull().sum())
df = df.replace(0, np.nan)
print('\n')

df = df.replace('NaN', np.nan)

#Filling missing values didn't give us the best results
#df['engV'].fillna(value=df['engV'].mean(),inplace=True)

#df.drive.value_counts()
#df['drive'].fillna(value='front',inplace=True)

df=df.dropna()
print('after deleting nulls')
print(df.isnull().sum())

"""# HeatMap and VIF"""

y=df["price"]
x=df.drop(["price"],1)

x["registration"] = pd.factorize(x['registration'])[0]
x["car"] = pd.factorize(x['car'])[0]
x["body"] = pd.factorize(x['body'])[0]
x["drive"] = pd.factorize(x['drive'])[0]
print(x)

features = '+'.join(x.columns)
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
vif["features"] = x.columns
vif.round(1)

correlation_matrix = x.corr().round(2)
sns.heatmap(data=correlation_matrix, annot=True)

"""# Split X,Y and preprocessing"""

y=df["price"]
x=df.drop(["price"],1)

categorical_f=['car','drive','body','registration']
numerical_f= list(x.drop(categorical_f, axis=1))
categorical_pipe = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
numerical_pipe = Pipeline([
    ("polynomial_features",PolynomialFeatures(2)),
    ("std_scaler", StandardScaler())
])

preprocessing = ColumnTransformer(
    [('cat', categorical_pipe, categorical_f),
     ('num', numerical_pipe, numerical_f)])

x = preprocessing.fit_transform(x)
print(x.shape)

#It didn't give us the best results (see graph at the presentation)
#x= SelectKBest(f_regression, k=70).fit_transform(x, y)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print(y_train)

"""# Metrics"""

resultsarray =[]

def metrics(predictions, y_t,name):
  errors = abs(predictions - y_t)
  mae=round(mean_absolute_error(y_t,predictions),2)
  print('Mean abs error: ', mae)
  maxe=max_error(predictions,y_t)
  print('Max eror: ',maxe)
  mape = 100 * (errors / y_t)
  # Calculate and display accuracy
  accuracy = 100 - np.mean(mape)
  acc=round(accuracy, 2)
  print('Accuracy:',acc , '%.')
  r2=r2_score(y_t, predictions)
  print('R2 ', r2)
  evs=explained_variance_score(predictions,y_t, multioutput='uniform_average')
  print('Explained variance score: ',evs)
  resultsarray.append([name,mae,maxe,acc,r2,evs])

"""# Random Forest Regressor"""

model = RandomForestRegressor(n_jobs=-1,n_estimators=98,random_state =0,verbose=True)
model.fit(X_train, y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Random Forest Regressor train')
print("TEST: \n")
pred=model.predict(X_test)
metrics(pred, y_test,'Random Forest Regressor test')

#feature importance can be used for RFR or DTR
feature_importances = model.feature_importances_ 
cat_encoder = categorical_pipe.named_steps["onehot"]
cat_one_hot_attribs = list(cat_encoder.categories[0])
attributes = numerical_f + categorical_f
sorted(zip(feature_importances, attributes), reverse=True)

"""# Decision Tree Regressor"""

model = DecisionTreeRegressor(random_state=0)
model.fit(X_train,y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Decision Tree Regressor train')
print("TEST: \n")
metrics( model.predict(X_test), y_test,'Decision Tree Regressor test')

"""# Linear Regression"""

model=LinearRegression()
model.fit(X_train,y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Linear Regression train')
print("TEST: \n")
metrics( model.predict(X_test), y_test,'Linear Regression test')

"""# SVR- Support Vector Machine Regression"""

#SVR with kernel = poly can't be used on the big dataset but it shows good results here
#so we kept it beside linearSVR we used on the big ds also
model = SVR(kernel='poly', epsilon=0.8, C=2000)
model.fit(X_train, y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'SVR train')
print("TEST: \n")
metrics( model.predict(X_test), y_test, 'SVR test')

model = LinearSVR(random_state=0,epsilon=0, C=2000)
model.fit(X_train, y_train)
print("TRAIN: \n")
metrics( model.predict(X_train), y_train,'Linear SVR train')
print("TEST: \n")
metrics( model.predict(X_test), y_test, 'Linear SVR test')

"""# Neural Network"""

pca = PCA(70)
X_train=X_train.toarray()
pca.fit(X_train)

X_train = pca.transform(X_train)
X_test=X_test.toarray()
X_test = pca.transform(X_test)
print(X_train.shape)

NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))

# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

NN_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)

# Load wights file of the best model :
wights_file = 'Weights-053--1938.99495.hdf5' # choose the best checkpoint 
NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

predictions = NN_model.predict(X_train)
print("TRAIN: \n")
metrics( predictions[:,0], y_train,'Neural Net train')
print("TEST: \n")
predictions = NN_model.predict(X_test)
metrics( predictions[:,0], y_test,'Neural Net test')

"""# Baseline"""

dummy = DummyRegressor(strategy="median")
dummy.fit(X_train, y_train)
predictions = dummy.predict(X_train)
print("TRAIN: \n")
metrics(dummy.predict(X_train), y_train, 'Dummy train')
print("TEST: \n")
metrics(dummy.predict(X_test), y_test, 'Dummy test')

"""# Results"""

results_df= pd.DataFrame(np.array(resultsarray),columns=['Model name','Mean abs error','Max error','Accuracy','R2','Explained variance score'])
results_df

"""# Grid Search"""

#GRID SEARCH for LinearSVR & PCA
X_train = X_train.toarray()
pipe = Pipeline([("pca",PCA()),("model",LinearSVR())])
grid = GridSearchCV(pipe, {'model__epsilon':[0.1,0,0.8],'model__C':[1, 2000,1000],'pca__n_components':[.95,50,90]})
grid.fit(X_train,y_train)
print('Best params on:\n')
print(grid.best_params_)

#GRID SEARCH for LinearSVR
grid = GridSearchCV(pipe, {'model__epsilon':[0.1,0,0.8],'model__C':[1, 2000,1000]})
grid.fit(X_train,y_train)
print('Best params on:\n')
print(grid.best_params_)

#GRID SEARCH for Random Forest Regressor
params = {
    'n_estimators': [150, 20],
   # 'min_samples_split': [2,4, 6],
    #'min_samples_leaf' : [3,2,1],
    'max_features':[10,20,'auto'],
    'max_depth':[20,50,100,'None']

}
               
grid = GridSearchCV(estimator = RandomForestRegressor(n_estimators=150,verbose=True),param_grid=params,cv = 2, verbose=True, n_jobs=-1, scoring='neg_mean_squared_error')
grid.fit(X_train,y_train)
print('Best params on:\n')
print(grid.best_params_)
print(grid.best_score_)

#GRID SEARCH for Decision tree regressor
max_features = ['auto', 50,20,70]
splitter=['best','random']
#min_samples_leaf = [1,3,50]
min_samples_split=[2,3,5]
max_depth =[50, 70,100]


random_grid = {'max_features': max_features,
              # 'splitter':splitter,
              # 'min_samples_leaf':min_samples_leaf,
               'min_samples_split':min_samples_split,
               #'max_depth':max_depth
               }
            

grid = GridSearchCV(estimator = DecisionTreeRegressor(), param_grid = random_grid, verbose=True,cv=3, n_jobs=-1)
grid.fit(X_train,y_train)
print('Best params on:\n')
print(grid.best_params_)
print(grid.best_score_)

pipe = Pipeline([("skb",SelectKBest()),("model",RandomForestRegressor())])

grid = GridSearchCV(pipe, {"skb__k": [8,10,15,20], "model__n_estimators":[70,80]})
grid.fit(X_train,y_train)
print('Best params on:\n')
print(grid.best_params_)

'''
pol=PolynomialFeatures()

pipe = Pipeline([('pol',pol), ('rfr', RandomForestRegressor(random_state=6))])
grid_search = GridSearchCV(pipe, { 'pol__degree': [1,2]})
grid_search.fit(X_train, y_train)
print('Best params on:\n')
print(grid_search.best_params_)
'''